{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 如何快速得近似任何函数\n",
    "This chapter uses neuralpy module, which is lastly supported by python 2.7 according to [pypy page](https://pypi.org/project/neuralpy/).\n",
    "\n",
    "I got too many errors that blocking me to use this module even under py27, so I gave up on this section.\n",
    "\n",
    "I mostly use python 3.5 as default version to code for these examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2016)\n",
    "sample_size=50\n",
    "sample = pd.Series(random.sample(range(-10000, 10000), sample_size))\n",
    "x = sample/10000\n",
    "y = x ** 2\n",
    "print(x.describe())\n",
    "\n",
    "# Type of pandas-Series\n",
    "print(type(sample))\n",
    "\n",
    "count = 0\n",
    "dataset = [([x.at[count]], [y.at[count]])]\n",
    "count = 1\n",
    "while (count < sample_size):\n",
    "    # print(\"Working on data item:\" + str(count))\n",
    "\n",
    "    # ix has been removed from pandas package, try iloc / loc / at\n",
    "    # dataset = (dataset + [([x.ix[count, 0]], [y.at[count]])])\n",
    "    dataset = (dataset + [([x.iloc[count]], [y.iloc[count]])])\n",
    "    \n",
    "    count = count + 1\n",
    "    \n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.x 如何构建可定制的深度预测模型\n",
    "This chapter uses scikit-neuralnetwork module, which is not included in anaconda by default, so at least you have to install it by yourself. Using the command line below:\n",
    "\n",
    "```\n",
    "pip install scikit-neuralnetwork\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "x, y = boston.data, boston.target\n",
    "\n",
    "# print(boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "x_MinMax = preprocessing.MinMaxScaler()\n",
    "y_MinMax = preprocessing .MinMaxScaler()\n",
    "\n",
    "# print(x)\n",
    "\n",
    "# turn array into a x * y matrix as you can leave 1 arguments unknown(i.e. -1) at most for creating it\n",
    "# if you want to create a matrix of 5 rows and don't know how many columns there are,\n",
    "# input 5 for args$1 and leave args$2 -1\n",
    "\n",
    "y = np.array(y).reshape((len(y), 1))\n",
    "# it's the same above and below\n",
    "y1 = np.array(y).reshape(-1,1)\n",
    "# print(y1)\n",
    "\n",
    "x = x_MinMax.fit_transform(x)\n",
    "y = y_MinMax.fit_transform(y)\n",
    "\n",
    "x.mean(axis = 0)\n",
    "# print(x)\n",
    "\n",
    "print(y_MinMax.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import train_test_split\n",
    "# sklearn has abandoned package cross_validation and replace it by model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(2016)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "\n",
    "# print(len(x_train))\n",
    "print(len(y_test))\n",
    "\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on higher version of python, you should have encounter a common error\n",
    "# ModuleNotFoundError: No module named 'sklearn.cross_validation'\n",
    "# somehow the mlp.py is not updated by maintainer that the import is not valid\n",
    "# go to where you install anaconda ~/anaconda3/Lib/sites-packages/sknn/mlp.py\n",
    "# edit import from sklearn.cross_validation to sklearn.model_selection\n",
    "from sknn.mlp import Regressor, Layer\n",
    "\n",
    "fit1 = Regressor (\n",
    "    layers = [\n",
    "        # 1st hidden layer\n",
    "        Layer(\"Sigmoid\", units = 6),\n",
    "        # 2nd hidden layer\n",
    "        Layer(\"Sigmoid\", units = 14),\n",
    "        # output layer\n",
    "        Layer(\"Linear\")\n",
    "    ],\n",
    "    learning_rate = 0.02,\n",
    "    random_state = 2016,\n",
    "    n_iter = 10\n",
    ")\n",
    "\n",
    "print(\"fitting model right now\")\n",
    "# you might encounter an error says '... from theano.tensor.signal import downsample'\n",
    "# ImportError: cannot import name 'downsample'\n",
    "# go to ~/anaconda3/envs/practice_env_0/Lib/site-packages/lasagne/layers/pool.py\n",
    "# module 'downsample' has been changed to 'pool' so you have to update it yourself\n",
    "# - from theano.tensor.signal import downsample\n",
    "# + from theano.tensor.signal import pool\n",
    "fit1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fit1.get_parameters())\n",
    "pred1_train = fit1.predict(x_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_1 = mean_squared_error(pred1_train, y_train)\n",
    "print(\"Train ERROR = \", mse_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b94d2afb",
   "metadata": {},
   "source": [
    "# 6.x 提高性能的一些技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50af3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reLU\n",
    "fit2 = Regressor (\n",
    "    layers = [\n",
    "    Layer(\"Rectifier\", units = 6),\n",
    "    Layer(\"Rectifier\", units = 14),\n",
    "    Layer(\"Linear\")\n",
    "    ],\n",
    "    learning_rate = 0.02,\n",
    "    random_state = 2016,\n",
    "    n_iter = 10\n",
    ")\n",
    "\n",
    "print(\"fitting model right now\")\n",
    "fit2.fit(x_train, y_train)\n",
    "pred2_train = fit2.predict(x_train)\n",
    "\n",
    "mse_2 = mean_squared_error(pred2_train, y_train)\n",
    "print(\"Train ERROR = \", mse_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48818f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set iter for 100 times\n",
    "fit3 = Regressor (\n",
    "    layers = [\n",
    "    Layer(\"Rectifier\", units = 6),\n",
    "    Layer(\"Rectifier\", units = 14),\n",
    "    Layer(\"Linear\")\n",
    "    ],\n",
    "    learning_rate = 0.02,\n",
    "    random_state = 2016,\n",
    "    n_iter = 100\n",
    ")\n",
    "\n",
    "print(\"fitting model right now\")\n",
    "fit3.fit(x_train, y_train)\n",
    "pred3_train = fit3.predict(x_train)\n",
    "\n",
    "mse_3 = mean_squared_error(pred3_train, y_train)\n",
    "print(\"Train ERROR = \", mse_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d60e590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model right now\n",
      "Train ERROR =  0.00738282398554\n"
     ]
    }
   ],
   "source": [
    "# regularization\n",
    "fit4 = Regressor (\n",
    "    layers = [\n",
    "    Layer(\"Rectifier\", units = 6),\n",
    "    Layer(\"Rectifier\", units = 14),\n",
    "    Layer(\"Linear\")\n",
    "    ],\n",
    "    learning_rate = 0.02,\n",
    "    random_state = 2016,\n",
    "    regularize = \"L2\",\n",
    "    weight_decay = 0.001,\n",
    "    n_iter = 100\n",
    ")\n",
    "\n",
    "print(\"fitting model right now\")\n",
    "fit4.fit(x_train, y_train)\n",
    "pred4_train = fit4.predict(x_train)\n",
    "\n",
    "mse_4 = mean_squared_error(pred4_train, y_train)\n",
    "print(\"Train ERROR = \", mse_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error function MSE\n",
    "pred4_test = fit4.predict(x_test)\n",
    "mse4_test = mean_squared_error(pred4_test, y_test)\n",
    "print(mse4_test)\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "correl = pearsonr(pred4_test, y_test)\n",
    "\n",
    "print(\"Test correlation is \", correl[0])\n",
    "print(\"Test R^2 is \", correl[0] * correl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze network weight\n",
    "fitFr = Regressor (\n",
    "    layers = [\n",
    "    Layer(\"Rectifier\", units = 6, frozen = True),\n",
    "    Layer(\"Rectifier\", units = 14, frozen = True),\n",
    "    Layer(\"Linear\")\n",
    "    ],\n",
    "    learning_rate = 0.02,\n",
    "    random_state = 2016,\n",
    "    regularize = \"L2\",\n",
    "    weight_decay = 0.001,\n",
    "    n_iter = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c815d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(fit4, open('./model/Boston_fit4.pkl', 'wb'))\n",
    "# pickle.dump(fit1, open('Boston_fit1.pkl', 'wb'))\n",
    "# pickle.dump(fit2, open('Boston_fit2.pkl', 'wb'))\n",
    "# pickle.dump(fit3, open('Boston_fit3.pkl', 'wb'))\n",
    "\n",
    "model = pickle.load(open('./model/Boston_fit4.pkl', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee5cf2a7",
   "metadata": {},
   "source": [
    "# 7.x 二元分类神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e34fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "url = \"http://goo.gl/j0Rvxq\"\n",
    "# if you have trouble accessing google, try download the dataset from datacastle\n",
    "# https://www.datacastle.cn/dataset_description.html?type=dataset&id=794\n",
    "# raw_data = urllib.urlopen(url)\n",
    "# urllib.urlopen have been moved to urllib.request.urlopen on py3.x\n",
    "raw_data = urllib.request.urlopen(url)\n",
    "dataset = np.loadtxt(raw_data, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# how to read data from csv file\n",
    "# def reader(master_table):\n",
    "# \tfile = 'data/' + master_table + '.csv'\n",
    "# \tdf = pd.read_csv(file,encoding='GB18030')\n",
    "# \tlabels = list(df.columns.values)\n",
    "# \treturn labels, df\n",
    "\n",
    "# call reader function\t\n",
    "# cols, raw_data = reader('diabetes')\n",
    "# print(cols)\n",
    "# for ix, r in raw_data.iterrows():\n",
    "# \tif(ix < 10):\n",
    "# \t    print(\"row\", ix, r['BMI'])\n",
    "\n",
    "# use function laodtxt is also good for csv files\n",
    "dataset = np.loadtxt('data/diabetes.csv', delimiter = \",\", skiprows = 1)\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "# transform to dataframe\n",
    "data = pd.DataFrame(dataset)\n",
    "# print(data.head(5))\n",
    "# replace 0 value by NaN(missing value)\n",
    "data = data.replace(0, np.nan)\n",
    "# print(data.head(5))\n",
    "# correct col 0 & 8 because these cols can have zero values(times, binary variable)\n",
    "data[0].fillna(0, inplace = True)\n",
    "# print(data.head(5))\n",
    "data[8].fillna(-1, inplace = True)\n",
    "# print(data.head(5))\n",
    "\n",
    "# solving missing value\n",
    "print(data.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
